{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bc7d921907d270"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, TrainingArguments, Trainer, AdamW, get_scheduler"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T04:21:10.112839Z",
     "start_time": "2025-03-27T04:21:10.107379Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# ----- Params -----\n",
    "\n",
    "data_dir = \"datasets/OldPeople_Voice/label/\"\n",
    "audio_dir = \"datasets/OldPeople_Voice/\"\n",
    "save_dir = \"whisper_finetuned\"\n",
    "\n",
    "# ----- ------ -----"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T04:30:43.050270Z",
     "start_time": "2025-03-27T04:30:43.030196Z"
    }
   },
   "id": "8fe0e23fa401af65"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, json_list, processor):\n",
    "        self.processor = processor\n",
    "        self.data = []\n",
    "\n",
    "        # 모든 JSON 파일을 리스트로\n",
    "        for json_path in json_list:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # 오디오 파일 경로\n",
    "            audio_file = os.path.join(audio_dir, data[\"발화정보\"][\"fileNm\"])\n",
    "            \n",
    "            # 파일이 실제 존재하는지 확인 (오류 방지)\n",
    "            if not os.path.exists(audio_file):\n",
    "                print(f\"⚠️ Warning: {audio_file} 파일이 존재하지 않습니다.\")\n",
    "                continue  # 해당 파일 건너뛰기\n",
    "            \n",
    "            text = data[\"발화정보\"][\"stt\"]\n",
    "            self.data.append((audio_file, text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, text = self.data[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        # 16kHz 샘플링 for Whisper\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "        # 오디오 데이터 변환\n",
    "        input_features = self.processor(\n",
    "            waveform.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "\n",
    "        # 텍스트 토큰화 하기\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features.squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T04:17:46.776708Z",
     "start_time": "2025-03-27T04:17:46.751199Z"
    }
   },
   "id": "bf2efaeef5ad26af"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    audio_file = os.path.join(audio_dir, data[\"발화정보\"][\"fileNm\"])\n",
    "    text = data[\"발화정보\"][\"stt\"]\n",
    "    # duration = float(str(data[\"발화정보\"][\"recrdTime\"]))\n",
    "\n",
    "    return {\n",
    "        \"audio\": audio_file,  # 파일 경로 저장\n",
    "        \"text\": text,\n",
    "        # \"duration\": duration\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T04:17:47.678863Z",
     "start_time": "2025-03-27T04:17:47.674121Z"
    }
   },
   "id": "ee9aa5a117860435"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 3\n",
    "gradient_accumulation_steps = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T04:29:35.117156Z",
     "start_time": "2025-03-27T04:29:35.071048Z"
    }
   },
   "id": "97b3e65d9e001913"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d066a3185eb47a783b3e89f92bad702"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7a44088ef65497a91083024ee8a2407"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data path : ['datasets/OldPeople_Voice/label/노인남여_노인대화77_F_김XX_62_제주_실내_84050.json', 'datasets/OldPeople_Voice/label/노인남여_노인대화77_F_김XX_62_제주_실내_84051.json', 'datasets/OldPeople_Voice/label/노인남여_노인대화77_F_김XX_62_제주_실내_84052.json', 'datasets/OldPeople_Voice/label/노인남여_노인대화77_F_김XX_62_제주_실내_84053.json', 'datasets/OldPeople_Voice/label/노인남여_노인대화77_F_김XX_62_제주_실내_84054.json']\n",
      "5개 로드.\n"
     ]
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "json_list = glob(data_dir+\"/*.json\")\n",
    "print(\"data path :\",json_list)\n",
    "dataset = CustomAudioDataset(json_list, processor)\n",
    "print(f\"{len(dataset)}개 로드.\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T06:51:21.632368Z",
     "start_time": "2025-03-27T06:49:28.405697Z"
    }
   },
   "id": "7ca657b529865494"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# optimizer & scheduler \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T06:51:24.338843Z",
     "start_time": "2025-03-27T06:51:24.233126Z"
    }
   },
   "id": "252abca12ecef155"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Epoch [1/3], Loss: 4.7223\n",
      "🚀 Epoch [2/3], Loss: 4.1509\n",
      "🚀 Epoch [3/3], Loss: 3.7681\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 배치에서 input_features와 labels 추출\n",
    "        input_features = [item[\"input_features\"].to(device) for item in batch]\n",
    "        labels = [item[\"labels\"].to(device) for item in batch]\n",
    "\n",
    "        # padding 처리\n",
    "        input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True, padding_value=0)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        # 모델에 입력\n",
    "        outputs = model(input_features, labels=labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps  # gradient accumulation 적용\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1 == len(train_dataloader)):\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"🚀 Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 모델 저장\n",
    "    model.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n",
    "    processor.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T06:56:17.038782Z",
     "start_time": "2025-03-27T06:52:04.725178Z"
    }
   },
   "id": "30f84c857290e6fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cb2850a0f1afc49b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b06d938581bc1970"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측된 텍스트: 어 큰 나들만 좋아하는 것 같아\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_path = \"whisper_finetuned/epoch_1\"  # X는 저장한 에포크 번호\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 오디오 파일\n",
    "audio_file = audio_dir + \"노인남여_노인대화77_F_김XX_62_제주_실내_84051.WAV\"\n",
    "\n",
    "# 오디오 파일 로드\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "# Whisper는 16kHz 샘플링 속도를 사용하므로 변환 필요\n",
    "if sample_rate != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "# 모델의 입력으로 변환\n",
    "input_features = processor(waveform.squeeze(0).numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "# 모델을 통해 예측 수행\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# 예측된 텍스트 디코딩\n",
    "transcribed_text = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"예측된 텍스트:\", transcribed_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T06:57:18.915038Z",
     "start_time": "2025-03-27T06:57:11.659954Z"
    }
   },
   "id": "c049e24048e1b03c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
