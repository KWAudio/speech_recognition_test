{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install torchaudio\n",
    "#! pip install tf-keras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bc7d921907d270"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import\\\n",
    "    torchaudio\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, TrainingArguments, Trainer, AdamW, get_scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:26:44.429957Z",
     "start_time": "2025-03-31T18:26:39.003131Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# ----- Params -----\n",
    "\n",
    "# 라벨 디렉터리\n",
    "# data_dir = \"datasets/OldPeople_Voice/label/\"\n",
    "data_dir = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\Training\\\\02.라벨링데이터\\\\\"\n",
    "# 오디오 디렉터리\n",
    "audio_dir = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\Training\\\\01.원천데이터\\\\\"\n",
    "# 학습된 데이터\n",
    "save_dir = \"whisper_finetuned\"\n",
    "\n",
    "# Validation label data\n",
    "validation_label = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\\"\n",
    "# Validation audio data\n",
    "validation_audio = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\\"\n",
    "\n",
    "# ----- ------ -----"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:27:02.453832Z",
     "start_time": "2025-03-31T18:27:02.443580Z"
    }
   },
   "id": "8fe0e23fa401af65"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, json_list, processor):\n",
    "        self.processor = processor\n",
    "        self.data = []\n",
    "\n",
    "        # 모든 JSON 파일을 리스트로\n",
    "        for json_path in json_list:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # 오디오 파일 경로\n",
    "            audio_file = os.path.join(audio_dir, data[\"fileName\"]+\".wav\")\n",
    "            \n",
    "            # 파일이 실제 존재하는지 확인 (오류 방지)\n",
    "            if not os.path.exists(audio_file):\n",
    "                print(f\"⚠️ Warning: {audio_file} 파일이 존재하지 않습니다.\")\n",
    "                continue  # 해당 파일 건너뛰기\n",
    "            \n",
    "            text = data[\"transcription\"][\"standard\"]\n",
    "            self.data.append((audio_file, text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, text = self.data[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        # 16kHz 샘플링 for Whisper\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "        # 오디오 데이터 변환\n",
    "        input_features = self.processor(\n",
    "            waveform.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "\n",
    "        # 텍스트 토큰화 하기\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features.squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:27:06.460306Z",
     "start_time": "2025-03-31T18:27:06.441856Z"
    }
   },
   "id": "bf2efaeef5ad26af"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    audio_file = os.path.join(audio_dir, data[\"fileName\"]+\".wav\")\n",
    "    text = data[\"transcription\"][\"standard\"]\n",
    "\n",
    "    return {\n",
    "        \"audio\": audio_file,  # 파일 경로 저장\n",
    "        \"text\": text,\n",
    "        # \"duration\": duration\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:27:08.409352Z",
     "start_time": "2025-03-31T18:27:08.391820Z"
    }
   },
   "id": "ee9aa5a117860435"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 3\n",
    "gradient_accumulation_steps = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:27:11.692669Z",
     "start_time": "2025-03-31T18:27:11.658799Z"
    }
   },
   "id": "97b3e65d9e001913"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 수 303149\n"
     ]
    }
   ],
   "source": [
    "json_list = glob(f\"{data_dir}**/*.json\", recursive=True)\n",
    "# print(sorted(json_list)[:10])\n",
    "print(\"data 수\",len(json_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:27:32.728978Z",
     "start_time": "2025-03-31T18:27:31.275867Z"
    }
   },
   "id": "caf4ffef4a757685",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m json_path \u001B[38;5;129;01min\u001B[39;00m json_list:\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(json_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m----> 8\u001B[0m             json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson 파일 데이터 무결성 검사 끝\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m json\u001B[38;5;241m.\u001B[39mJSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# JSON file 검사\n",
    "# 라벨 데이터에 문제 발생 시... 직접 수정 바랍니다.\n",
    "# 139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\01-1.정식개방데이터\\Training\\02.라벨링데이터\\TL_02. 경상도_01. 1인발화 따라말하기\\st_set1_collectorgs100_speakergs442_54_10 에서 , 중복 문제 있었음!\n",
    "json_path = \"\"\n",
    "try:\n",
    "    for json_path in json_list:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json.load(f)\n",
    "    print(\"json 파일 데이터 무결성 검사 끝\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(json_path,\", label data 오류발생!!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:29:43.405301Z",
     "start_time": "2025-03-31T18:27:37.959472Z"
    }
   },
   "id": "cb1fa11e93361b95",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "processor = WhisperProcessor.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:41:54.985593Z",
     "start_time": "2025-03-31T18:41:51.513709Z"
    }
   },
   "id": "17fc863a270ddbc1",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303149개 로드.\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomAudioDataset(json_list, processor)\n",
    "print(f\"{len(dataset)}개 로드.\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "print(f\"steps 총 {len(train_dataloader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:33:17.370103Z",
     "start_time": "2025-03-31T18:29:52.471476Z"
    }
   },
   "id": "7ca657b529865494"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haiiron\\.conda\\envs\\cuda_38\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer & scheduler \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T18:34:57.443623Z",
     "start_time": "2025-03-31T18:34:57.428223Z"
    }
   },
   "id": "252abca12ecef155"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haiiron\\.conda\\envs\\cuda_38\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error at step 89: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 180: Labels' sequence length 463 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 222: Labels' sequence length 498 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 251: Labels' sequence length 492 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 285: Labels' sequence length 512 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 329: Labels' sequence length 453 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 348: Labels' sequence length 450 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 480: Labels' sequence length 559 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 587: Labels' sequence length 494 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 614: Labels' sequence length 494 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 710: Labels' sequence length 484 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 711: Labels' sequence length 485 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 741: Labels' sequence length 470 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 745: Labels' sequence length 452 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 765: Labels' sequence length 459 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 784: Labels' sequence length 457 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 873: Labels' sequence length 534 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 877: Labels' sequence length 506 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 956: Labels' sequence length 473 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 991: Labels' sequence length 561 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1034: Labels' sequence length 470 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1035: Labels' sequence length 600 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1080: Labels' sequence length 489 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1130: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1195: Labels' sequence length 501 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1344: Labels' sequence length 452 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1541: Labels' sequence length 462 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1599: Labels' sequence length 487 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1623: Labels' sequence length 580 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1672: Labels' sequence length 460 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1677: Labels' sequence length 455 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1757: Labels' sequence length 462 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1782: Labels' sequence length 503 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1848: Labels' sequence length 517 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1867: Labels' sequence length 498 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1871: Labels' sequence length 510 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1892: Labels' sequence length 491 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1945: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1965: Labels' sequence length 508 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1984: Labels' sequence length 479 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 1999: Labels' sequence length 449 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2009: Labels' sequence length 487 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2014: Labels' sequence length 478 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2076: Labels' sequence length 465 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2195: Labels' sequence length 458 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2204: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2207: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2260: Labels' sequence length 468 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2274: Labels' sequence length 499 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2302: Labels' sequence length 451 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2312: Labels' sequence length 512 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2438: Labels' sequence length 453 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2603: Labels' sequence length 528 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2635: Labels' sequence length 455 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2731: Labels' sequence length 464 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2739: Labels' sequence length 459 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2745: Labels' sequence length 478 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2811: Labels' sequence length 471 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2868: Labels' sequence length 485 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2897: Labels' sequence length 463 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2935: Labels' sequence length 505 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 2991: Labels' sequence length 521 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3075: Labels' sequence length 460 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3188: Labels' sequence length 508 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3203: Labels' sequence length 588 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3238: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3371: Labels' sequence length 458 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3403: Labels' sequence length 470 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3460: Labels' sequence length 520 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3578: Labels' sequence length 526 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3587: Labels' sequence length 509 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3654: Labels' sequence length 459 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3689: Labels' sequence length 494 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3722: Labels' sequence length 453 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3763: Labels' sequence length 547 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3781: Labels' sequence length 469 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3798: Labels' sequence length 614 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3943: Labels' sequence length 500 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 3967: Labels' sequence length 505 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4065: Labels' sequence length 457 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4078: Labels' sequence length 487 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4139: Labels' sequence length 467 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4312: Labels' sequence length 487 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4318: Labels' sequence length 524 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4343: Labels' sequence length 479 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4381: Labels' sequence length 473 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4383: Labels' sequence length 470 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4554: Labels' sequence length 454 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4568: Labels' sequence length 496 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4603: Labels' sequence length 459 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4609: Labels' sequence length 458 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4677: Labels' sequence length 528 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4715: Labels' sequence length 479 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4851: Labels' sequence length 451 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4980: Labels' sequence length 473 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 4999: Labels' sequence length 449 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5000: Labels' sequence length 506 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5053: Labels' sequence length 450 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5118: Labels' sequence length 530 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5147: Labels' sequence length 562 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5155: Labels' sequence length 527 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5160: Labels' sequence length 453 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5175: Labels' sequence length 456 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5302: Labels' sequence length 450 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5313: Labels' sequence length 502 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5362: Labels' sequence length 515 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5480: Labels' sequence length 585 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5518: Labels' sequence length 511 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5546: Labels' sequence length 488 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5605: Labels' sequence length 496 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5611: Labels' sequence length 480 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5619: Labels' sequence length 487 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5741: Labels' sequence length 529 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5772: Labels' sequence length 481 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5912: Labels' sequence length 456 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5960: Labels' sequence length 545 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 5987: Labels' sequence length 518 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6020: Labels' sequence length 464 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6027: Labels' sequence length 467 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6055: Labels' sequence length 575 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6134: Labels' sequence length 480 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6189: Labels' sequence length 510 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6247: Labels' sequence length 480 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6264: Labels' sequence length 492 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6269: Labels' sequence length 494 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6286: Labels' sequence length 479 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6304: Labels' sequence length 467 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6314: Labels' sequence length 599 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6356: Labels' sequence length 487 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6420: Labels' sequence length 466 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6436: Labels' sequence length 554 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6478: Labels' sequence length 449 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6483: Labels' sequence length 522 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6520: Labels' sequence length 514 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6585: Labels' sequence length 459 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6597: Labels' sequence length 625 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6611: Labels' sequence length 474 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6624: Labels' sequence length 524 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6629: Labels' sequence length 486 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6740: Labels' sequence length 665 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6751: Labels' sequence length 513 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6824: Labels' sequence length 478 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6826: Labels' sequence length 458 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6930: Labels' sequence length 471 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 6932: Labels' sequence length 462 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7032: Labels' sequence length 491 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7054: Labels' sequence length 490 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7058: Labels' sequence length 507 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7256: Labels' sequence length 497 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7271: Labels' sequence length 463 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7322: Labels' sequence length 451 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7353: Labels' sequence length 484 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7366: Labels' sequence length 457 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7378: Labels' sequence length 485 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7455: Labels' sequence length 527 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7533: Labels' sequence length 571 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7537: Labels' sequence length 521 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7558: Labels' sequence length 495 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7659: Labels' sequence length 449 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7679: Labels' sequence length 470 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7791: Labels' sequence length 472 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7792: Labels' sequence length 463 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 7917: Labels' sequence length 454 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8022: Labels' sequence length 540 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8036: Labels' sequence length 458 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8057: Labels' sequence length 513 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8134: Labels' sequence length 471 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8187: Labels' sequence length 463 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8203: Labels' sequence length 494 cannot exceed the maximum allowed length of 448 tokens.\n",
      "⚠️ Error at step 8252: Labels' sequence length 455 cannot exceed the maximum allowed length of 448 tokens.\n"
     ]
    }
   ],
   "source": [
    "error_count = 0  # 에러 발생 횟수 저장\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        try:\n",
    "            print(\"try \", step)\n",
    "            # 배치에서 input_features와 labels 추출\n",
    "            input_features = [item[\"input_features\"].to(device) for item in batch]\n",
    "            labels = [item[\"labels\"].to(device) for item in batch]\n",
    "\n",
    "            # padding 처리\n",
    "            input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True, padding_value=0)\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "            # 모델에 입력\n",
    "            outputs = model(input_features, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # gradient accumulation 적용\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1 == len(train_dataloader)):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"️ Error at step {step}: {e}\")\n",
    "            continue  # 에러 발생 시 건너뛰기\n",
    "\n",
    "    avg_loss = total_loss / (len(train_dataloader) - error_count)\n",
    "    print(f\"🚀 Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Errors: {error_count}\")\n",
    "\n",
    "    # 모델 저장\n",
    "    model.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n",
    "    processor.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-03-31T18:42:08.378604Z"
    }
   },
   "id": "30f84c857290e6fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cb2850a0f1afc49b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b06d938581bc1970"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_path = \"whisper_finetuned/epoch_1\"  # X는 저장한 에포크 번호\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 오디오 파일\n",
    "audio_file = audio_dir + \"노인남여_노인대화77_F_김XX_62_제주_실내_84051.WAV\"\n",
    "\n",
    "# 오디오 파일 로드\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "# Whisper는 16kHz 샘플링 속도를 사용하므로 변환 필요\n",
    "if sample_rate != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "# 모델의 입력으로 변환\n",
    "input_features = processor(waveform.squeeze(0).numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "# 모델을 통해 예측 수행\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# 예측된 텍스트 디코딩\n",
    "transcribed_text = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"예측된 텍스트:\", transcribed_text)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c049e24048e1b03c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
